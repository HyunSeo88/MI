"""
Configuration Management for Super Resolution Training
SR í•™ìŠµì„ ìœ„í•œ ì„¤ì • ê´€ë¦¬
"""

import argparse
import json
import os
from dataclasses import dataclass, asdict
from typing import Optional, List


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •"""
    # ëª¨ë¸ ì„¤ì •
    model_name: str = 'EDSR'
    scale: int = 2
    
    # ë°ì´í„° ì„¤ì •
    data_path: str = './data/DIV2K'
    patch_size: int = 96
    batch_size: int = 16
    num_workers: int = 0  # Windows í˜¸í™˜ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ê¸°ë³¸ ì„¤ì •
    
    # í•™ìŠµ ì„¤ì •
    epochs: int = 100
    learning_rate: float = 1e-4
    weight_decay: float = 1e-4
    loss_type: str = 'L1'  # L1, L2, MSE, Huber
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer: str = 'Adam'  # Adam, AdamW, SGD
    scheduler: str = 'CosineAnnealingLR'  # CosineAnnealingLR, StepLR, MultiStepLR, None
    
    # ë©”íŠ¸ë¦­ ì„¤ì •
    use_psnr: bool = True
    use_ssim: bool = True
    
    # ê¸°íƒ€ ì„¤ì •
    device: str = 'auto'  # auto, cpu, cuda
    experiment_name: Optional[str] = None
    resume_from: Optional[str] = None
    save_every: int = 10  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°
    eval_every: int = 1   # í‰ê°€ ì£¼ê¸°
    
    # ë¡œê¹… ì„¤ì •
    log_level: str = 'INFO'
    save_images: bool = True
    save_curves: bool = True
    
    # ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì •
    model_params: dict = None
    
    def __post_init__(self):
        if self.model_params is None:
            self.model_params = {}
        
        if self.experiment_name is None:
            self.experiment_name = f"{self.model_name}_x{self.scale}"


def get_default_model_params(model_name: str, scale: int = 2) -> dict:
    """
    ëª¨ë¸ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë°˜í™˜
    
    Args:
        model_name (str): ëª¨ë¸ ì´ë¦„
        scale (int): í™•ëŒ€ ë¹„ìœ¨
        
    Returns:
        dict: ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    """
    params = {
        'EDSR': {
            'n_colors': 3,
            'n_feats': 64,
            'kernel_size': 3,
            'n_resblocks': 16,
            'scale': scale,
            'res_scale': 1.0
        },
        'RCAN': {
            'n_colors': 3,
            'n_feats': 64,
            'kernel_size': 3,
            'n_resblocks': 20,
            'scale': scale
        },
        'ESRGAN': {
            'n_colors': 3,
            'n_feats': 64,
            'kernel_size': 3,
            'n_basic_blocks': 23,
            'scale': scale
        },
        'SwinIR': {
            'img_size': 64,
            'patch_size': 1,
            'in_chans': 3,
            'embed_dim': 96,
            'depths': [6, 6, 6, 6],
            'num_heads': [6, 6, 6, 6],
            'window_size': 7,
            'mlp_ratio': 4.0,
            'upscale': scale
        }
    }
    
    return params.get(model_name, {'scale': scale})


def create_argument_parser():
    """
    ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì„œ ìƒì„±
    
    Returns:
        argparse.ArgumentParser: íŒŒì„œ
    """
    parser = argparse.ArgumentParser(
        description='Super Resolution Model Training',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument('--model', type=str, default='EDSR',
                       choices=['EDSR', 'RCAN', 'ESRGAN', 'SwinIR'],
                       help='Model architecture')
    parser.add_argument('--scale', type=int, default=2,
                       choices=[2, 3, 4, 8],
                       help='Super resolution scale factor')
    
    # ë°ì´í„° ê´€ë ¨
    parser.add_argument('--data-path', type=str, default='./data/DIV2K',
                       help='Path to dataset')
    parser.add_argument('--patch-size', type=int, default=96,
                       help='HR patch size for training')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='Batch size')
    parser.add_argument('--num-workers', type=int, default=0,
                       help='Number of data loading workers')
    
    # í•™ìŠµ ê´€ë ¨
    parser.add_argument('--epochs', type=int, default=100,
                       help='Number of training epochs')
    parser.add_argument('--lr', '--learning-rate', type=float, default=1e-4,
                       help='Learning rate')
    parser.add_argument('--weight-decay', type=float, default=1e-4,
                       help='Weight decay')
    parser.add_argument('--loss', type=str, default='L1',
                       choices=['L1', 'L2', 'MSE', 'Huber'],
                       help='Loss function')
    
    # ì˜µí‹°ë§ˆì´ì € ê´€ë ¨
    parser.add_argument('--optimizer', type=str, default='Adam',
                       choices=['Adam', 'AdamW', 'SGD'],
                       help='Optimizer')
    parser.add_argument('--scheduler', type=str, default='CosineAnnealingLR',
                       choices=['CosineAnnealingLR', 'StepLR', 'MultiStepLR', 'None'],
                       help='Learning rate scheduler')
    
    # ë©”íŠ¸ë¦­ ê´€ë ¨
    parser.add_argument('--no-psnr', action='store_true',
                       help='Disable PSNR calculation')
    parser.add_argument('--no-ssim', action='store_true',
                       help='Disable SSIM calculation')
    
    # ê¸°íƒ€
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda'],
                       help='Device to use')
    parser.add_argument('--experiment-name', type=str, default=None,
                       help='Experiment name')
    parser.add_argument('--resume-from', type=str, default=None,
                       help='Resume training from checkpoint')
    parser.add_argument('--save-every', type=int, default=10,
                       help='Save checkpoint every N epochs')
    parser.add_argument('--eval-every', type=int, default=1,
                       help='Evaluate every N epochs')
    
    # ë¡œê¹… ê´€ë ¨
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='Logging level')
    parser.add_argument('--no-save-images', action='store_true',
                       help='Disable saving sample images')
    parser.add_argument('--no-save-curves', action='store_true',
                       help='Disable saving training curves')
    
    # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° (JSON í˜•íƒœë¡œ ì…ë ¥)
    parser.add_argument('--model-params', type=str, default=None,
                       help='Model parameters as JSON string')
    
    return parser


def parse_config_from_args(args) -> TrainingConfig:
    """
    ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œë¶€í„° ì„¤ì • ê°ì²´ ìƒì„±
    
    Args:
        args: argparse ê²°ê³¼
        
    Returns:
        TrainingConfig: ì„¤ì • ê°ì²´
    """
    # ëª¨ë¸ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
    model_params = get_default_model_params(args.model, args.scale)
    
    # ì‚¬ìš©ì ì •ì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ ì˜¤ë²„ë¼ì´ë“œ
    if args.model_params:
        try:
            custom_params = json.loads(args.model_params)
            model_params.update(custom_params)
        except json.JSONDecodeError:
            print(f"Warning: Invalid JSON in --model-params: {args.model_params}")
    
    config = TrainingConfig(
        model_name=args.model,
        scale=args.scale,
        data_path=args.data_path,
        patch_size=args.patch_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        epochs=args.epochs,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        loss_type=args.loss,
        optimizer=args.optimizer,
        scheduler=args.scheduler if args.scheduler != 'None' else None,
        use_psnr=not args.no_psnr,
        use_ssim=not args.no_ssim,
        device=args.device,
        experiment_name=args.experiment_name,
        resume_from=args.resume_from,
        save_every=args.save_every,
        eval_every=args.eval_every,
        log_level=args.log_level,
        save_images=not args.no_save_images,
        save_curves=not args.no_save_curves,
        model_params=model_params
    )
    
    return config


def save_config(config: TrainingConfig, filepath: str):
    """
    ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        config (TrainingConfig): ì„¤ì • ê°ì²´
        filepath (str): ì €ì¥ ê²½ë¡œ
    """
    config_dict = asdict(config)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=4, ensure_ascii=False)
    
    print(f"Config saved to: {filepath}")


def load_config(filepath: str) -> TrainingConfig:
    """
    JSON íŒŒì¼ë¡œë¶€í„° ì„¤ì • ë¡œë“œ
    
    Args:
        filepath (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        TrainingConfig: ì„¤ì • ê°ì²´
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Config file not found: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    return TrainingConfig(**config_dict)


def print_config(config: TrainingConfig):
    """
    ì„¤ì • ì •ë³´ ì¶œë ¥
    
    Args:
        config (TrainingConfig): ì„¤ì • ê°ì²´
    """
    print("\n" + "="*60)
    print("TRAINING CONFIGURATION")
    print("="*60)
    
    print(f"ğŸ“‹ Experiment: {config.experiment_name}")
    print(f"ğŸ—ï¸  Model: {config.model_name} (x{config.scale})")
    print(f"ğŸ“Š Dataset: {config.data_path}")
    print(f"ğŸ¯ Patch Size: {config.patch_size}")
    print(f"ğŸ“¦ Batch Size: {config.batch_size}")
    print(f"ğŸ”„ Epochs: {config.epochs}")
    print(f"ğŸ“ˆ Learning Rate: {config.learning_rate}")
    print(f"ğŸ”§ Optimizer: {config.optimizer}")
    print(f"ğŸ“‰ Scheduler: {config.scheduler}")
    print(f"ğŸ’¾ Loss Function: {config.loss_type}")
    print(f"ğŸ“ Metrics: PSNR={config.use_psnr}, SSIM={config.use_ssim}")
    print(f"âš™ï¸  Device: {config.device}")
    
    if config.model_params:
        print(f"\nğŸ—ï¸  Model Parameters:")
        for key, value in config.model_params.items():
            print(f"   {key}: {value}")
    
    print("="*60)


def validate_config(config: TrainingConfig) -> List[str]:
    """
    ì„¤ì • ê²€ì¦
    
    Args:
        config (TrainingConfig): ì„¤ì • ê°ì²´
        
    Returns:
        List[str]: ì—ëŸ¬ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ë¹„ì–´ìˆìœ¼ë©´ ì •ìƒ)
    """
    errors = []
    
    # í•„ìˆ˜ ê²½ë¡œ ê²€ì¦
    if not os.path.exists(config.data_path):
        errors.append(f"Data path not found: {config.data_path}")
    
    # ê°’ ë²”ìœ„ ê²€ì¦
    if config.batch_size <= 0:
        errors.append("Batch size must be positive")
    
    if config.epochs <= 0:
        errors.append("Epochs must be positive")
    
    if config.learning_rate <= 0:
        errors.append("Learning rate must be positive")
    
    if config.scale not in [2, 3, 4, 8]:
        errors.append("Scale must be one of [2, 3, 4, 8]")
    
    if config.patch_size <= 0:
        errors.append("Patch size must be positive")
    
    # Resume íŒŒì¼ ê²€ì¦
    if config.resume_from and not os.path.exists(config.resume_from):
        errors.append(f"Resume checkpoint not found: {config.resume_from}")
    
    return errors


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("Configuration Management Test")
    
    # ê¸°ë³¸ ì„¤ì • ìƒì„±
    config = TrainingConfig()
    print_config(config)
    
    # ì„¤ì • ê²€ì¦
    errors = validate_config(config)
    if errors:
        print("\nâŒ Configuration errors:")
        for error in errors:
            print(f"  - {error}")
    else:
        print("\nâœ… Configuration is valid")
    
    # ëª…ë ¹í–‰ íŒŒì„œ í…ŒìŠ¤íŠ¸
    parser = create_argument_parser()
    
    # ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶œë ¥
    print("\nğŸ“‹ Default Model Parameters:")
    for model in ['EDSR', 'RCAN', 'ESRGAN', 'SwinIR']:
        params = get_default_model_params(model, scale=2)
        print(f"  {model}: {params}")
    
    print("\nğŸš€ Use this module with train.py for full functionality!") 